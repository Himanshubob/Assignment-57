{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31beb9aa",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1bf9a",
   "metadata": {},
   "source": [
    "# ans.. gredient boost regression is an technic for the  boosting algo   this tecahnic solve the regression problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95dcf4",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8275cc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 49.30993186715702\n",
      "R-squared: -13.542860441073156\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of the target variable\n",
    "        pred = np.mean(y)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the pseudo-residuals\n",
    "            residuals = y - pred\n",
    "            \n",
    "            # Fit a weak learner (Decision Tree) to the pseudo-residuals\n",
    "            model = DecisionTreeRegressor(max_depth=3)\n",
    "            model.fit(X, residuals)\n",
    "            \n",
    "            # Update predictions with a fraction of the new model's predictions\n",
    "            pred += self.learning_rate * model.predict(X)\n",
    "            \n",
    "            # Save the model\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing up the predictions from all models\n",
    "        pred = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            pred += self.learning_rate * model.predict(X)\n",
    "        return pred\n",
    "\n",
    "# Generate a small dataset for demonstration\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_model.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e8567",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa8a42",
   "metadata": {},
   "source": [
    "# There are various hyperparameters that can be controlled in a random forest: N_estimators: The number of decision trees being built in the forest. Default values in sklearn are 100. N_estimators are mostly correlated to the size of data, to encapsulate the trends in the data, more number of DTs are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb02c2f",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62062128",
   "metadata": {},
   "source": [
    "# A single model, also known as a base or weak learner, may not perform well individually due to high variance or high bias. However, when weak learners are aggregated, they can form a strong learner, as their combination reduces bias or variance, yielding better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12acf50",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa5ef5",
   "metadata": {},
   "source": [
    "# ans.. gradient boosting algo first creat the base leanner and after creat the DT  and many DT  Basie learnner data provide the DT and DT predected and the Data provied the next DT.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448da85",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b0f76",
   "metadata": {},
   "source": [
    "# The gradient-boosting regressor works by iteratively building an ensemble of weak learners, where each subsequent weak learner is trained to correct the mistakes made by the previous ones. The predictions from all weak learners are combined to make the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc89dca0",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a2556",
   "metadata": {},
   "source": [
    "# Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c13746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
